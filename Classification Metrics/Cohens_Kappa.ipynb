{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f98951f",
   "metadata": {},
   "source": [
    "\n",
    "# 🎯 Cohen's Kappa Nedir?\n",
    "\n",
    "Cohen's Kappa, iki değerlendirici veya iki sınıflandırıcı arasındaki **uyumu** ölçmek için kullanılan bir metriktir.\n",
    "\n",
    "Sadece yüzeysel uyumu (yani, yüzde kaç aynı sonucu verdiler) değil, **rastgele aynı tahmin etme olasılığını da dikkate alır**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📍 Nerede Kullanılır?\n",
    "\n",
    "- Tıp alanında iki doktorun aynı teşhisi koyma oranını ölçmek\n",
    "- Veri etiketleme sürecinde etiketleyiciler arasındaki uyumu kontrol etmek\n",
    "- Bir modelin insan etiketiyle ne kadar uyumlu olduğunu görmek\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Avantajları\n",
    "\n",
    "✅ Rastgele uyumu hesaba katar  \n",
    "✅ Kategorik (sınıflandırma) verilerde anlamlı\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Dezavantajları\n",
    "\n",
    "❌ Sadece **iki değerlendirici** için uygundur  \n",
    "❌ Dengesiz veri setlerinde yorumlamak zor olabilir\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Değer Aralığı\n",
    "\n",
    "- **1.0**: Tam uyum\n",
    "- **0.0**: Rastgele uyum\n",
    "- **< 0.0**: Rastgeleden daha kötü uyum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1ffb6",
   "metadata": {},
   "source": [
    "\n",
    "## ✨ Cohen's Kappa Formülü\n",
    "\n",
    "Cohen's Kappa şöyle hesaplanır:\n",
    "\n",
    "Kappa = (Po - Pe) / (1 - Pe)\n",
    "\n",
    "Burada:\n",
    "\n",
    "- **pₒ (observed agreement)**: Gözlemlenen (gerçek) uyum oranıdır. Yani, iki değerlendiricinin aynı kararı verdiği örneklerin oranıdır.\n",
    "- **pₑ (expected agreement)**: Rastgele aynı kararı verme olasılığıdır.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Formülün Yorumu\n",
    "\n",
    "- Eğer değerlendiriciler tamamen aynı kararı verirse \\( p_o = 1 \\) olur, bu durumda \\( \\kappa = 1 \\).\n",
    "- Eğer uyum tamamen rastgele ise, \\( p_o = p_e \\), bu durumda \\( \\kappa = 0 \\).\n",
    "- Eğer \\( p_o < p_e \\), yani uyum rastgeleden daha kötüyse, \\( \\kappa < 0 \\).\n",
    "\n",
    "Bu formül sayesinde rastgele tahminin etkisi ortadan kaldırılır ve **gerçek uyum kalitesi** ölçülür.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b735b",
   "metadata": {},
   "source": [
    "\n",
    "## 👨‍⚖️ Değerlendirici Etiketleri Ne Anlama Gelir?\n",
    "\n",
    "- **Birinci değerlendirici (true_labels):** Gerçek sınıflar (ground truth) veya ilk insan etiketi.\n",
    "- **İkinci değerlendirici (pred_labels):** Modelin tahmini ya da başka bir insan etiketi.\n",
    "\n",
    "Cohen's Kappa, iki değerlendiricinin aynı örneğe ne kadar benzer karar verdiğini ölçer. Örneğin bir doktor ve bir modelin teşhis uyumu, ya da iki doktorun teşhisi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268994d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c22592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Skoru: 0.627906976744186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gerçek etiketler veya ilk değerlendiricinin etiketleri\n",
    "true_labels = [1, 0, 2, 1, 0, 2, 2, 1]\n",
    "\n",
    "# İkinci değerlendiricinin (veya modelin) tahminleri\n",
    "pred_labels = [1, 0, 2, 0, 0, 2, 1, 1]\n",
    "\n",
    "# Cohen's Kappa skorunu hesaplayalım\n",
    "kappa = cohen_kappa_score(true_labels, pred_labels)\n",
    "\n",
    "# Sonucu ekrana yazdıralım\n",
    "print(\"Cohen's Kappa Skoru:\", kappa)\n",
    "\n",
    "# 📌 Açıklama:\n",
    "# true_labels: Gerçek sınıflar (veya birinci değerlendirici)\n",
    "# pred_labels: İkinci değerlendirici veya model tahminleri\n",
    "# cohen_kappa_score(): İki etiket seti arasındaki kappa skorunu döndürür\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3179f2",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 Sonuç\n",
    "\n",
    "Bu örnekte hesapladığımız Cohen's Kappa skoru, iki değerlendirici (veya model ile gerçek etiket) arasındaki **uyumun derecesini** gösterir.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Skor Değerlendirmesi\n",
    "\n",
    "Bu örnekte çıkan **0.6279** civarındaki skor, **iyi düzeyde bir uyuma** işaret eder. Genel olarak kappa skorları şöyle yorumlanabilir:\n",
    "\n",
    "- **< 0.00**: Kötü uyum (rastgele tahminden daha kötü)\n",
    "- **0.00 – 0.20**: Zayıf uyum\n",
    "- **0.21 – 0.40**: Düşük uyum\n",
    "- **0.41 – 0.60**: Orta düzey uyum\n",
    "- **0.61 – 0.80**: İyi uyum\n",
    "- **0.81 – 1.00**: Çok iyi (mükemmel) uyum\n",
    "\n",
    "Bu durumda skorumuz **0.6279**, \"iyi\" kategorisine girer ve değerlendiriciler arasında anlamlı bir uyum olduğunu gösterir.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Özet\n",
    "\n",
    "- Cohen's Kappa, iki sınıflandırıcı arasındaki uyumu ölçmek için güçlü bir metriktir.\n",
    "- Tesadüfi uyum ihtimalini göz önünde bulundurur.\n",
    "- Özellikle etikete dayalı çalışmalarda tercih edilir.\n",
    "\n",
    "---\n",
    "\n",
    "🎁 **İpucu:** Dengesiz veri setleriniz varsa, Cohen's Kappa skorunun düşük çıkabileceğini ve bunun her zaman kötü performans anlamına gelmediğini unutmayın.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
